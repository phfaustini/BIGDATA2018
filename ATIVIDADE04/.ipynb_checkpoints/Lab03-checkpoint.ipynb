{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CMCC](http://cmcc.ufabc.edu.br/images/logo_site.jpg)\n",
    "# **Modelos de Classificação**\n",
    "\n",
    "#### Este laboratório irá cobrir os passos para tratar a base de dados de taxa de cliques (click-through rate - CTR) e criar um modelo de classificação para tentar determinar se um usuário irá ou não clicar em um banner.\n",
    "\n",
    "#### Para isso utilizaremos a base de dados [Criteo Labs](http://labs.criteo.com/) que foi utilizado em uma competição do [Kaggle](https://www.kaggle.com/c/criteo-display-ad-challenge).\n",
    "\n",
    "#### ** Nesse notebook: **\n",
    "+  #### *Parte 1:* Utilização do one-hot-encoding (OHE) para transformar atributos categóricos em numéricos\n",
    "+  #### *Parte 2:* Construindo um dicionário OHE\n",
    "+  #### *Parte 3:* Geração de atributos OHE na base de dados CTR\n",
    " + #### *Visualização 1:* Frequência de atributos\n",
    "+  #### *Parte 4:* Predição de CTR e avaliação da perda logarítimica (logloss)\n",
    " + #### *Visualização 2:* Curva ROC\n",
    "+  #### *Parte 5:* Reduzindo a dimensão dos atributos através de hashing (feature hashing)\n",
    " \n",
    "#### Referências de métodos: [Spark's Python API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)e [NumPy Reference](http://docs.scipy.org/doc/numpy/reference/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 1: Utilização do one-hot-encoding (OHE) para transformar atributos categóricos em numéricos **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1a) One-hot-encoding **\n",
    "\n",
    "#### Para um melhor entendimento do processo da codificação OHE vamos trabalhar com uma base de dados pequena e sem rótulos. Cada objeto dessa base pode conter três atributos, o primeiro indicando o animal, o segundo a cor e o terceiro qual animal que ele come.\n",
    "\n",
    "#### No esquema OHE, queremos representar cada tupla `(IDatributo, categoria)` através de um atributo binário. Nós podemos fazer isso no Python criando um dicionário que mapeia cada possível tupla em um inteiro que corresponde a sua posição no vetor de atributos binário.\n",
    "\n",
    "#### Para iniciar crie um dicionário correspondente aos atributos categóricos da base construída logo abaixo. Faça isso manualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles, SparkContext\n",
    "sc = SparkContext().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for manual OHE\n",
    "# Note: the first data point does not include any value for the optional third feature\n",
    "sampleOne = [(0, 'mouse'), (1, 'black')]\n",
    "sampleTwo = [(0, 'cat'), (1, 'tabby'), (2, 'mouse')]\n",
    "sampleThree =  [(0, 'bear'), (1, 'black'), (2, 'salmon')]\n",
    "sampleDataRDD = sc.parallelize([sampleOne, sampleTwo, sampleThree])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "sampleOHEDictManual = {}\n",
    "sampleOHEDictManual[sampleOne[0]] = 0\n",
    "sampleOHEDictManual[sampleOne[1]] = 1\n",
    "sampleOHEDictManual[sampleTwo[0]] = 0\n",
    "sampleOHEDictManual[sampleTwo[1]] = 1\n",
    "sampleOHEDictManual[sampleTwo[2]] = 2\n",
    "sampleOHEDictManual[sampleThree[0]] = 0\n",
    "sampleOHEDictManual[sampleThree[1]] = 1\n",
    "sampleOHEDictManual[sampleThree[2]] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST One-hot-encoding (1a)\n",
    "assert (0, 'mouse') in sampleOHEDictManual, \"(0, 'mouse') not in sampleOHEDictManual\"\n",
    "assert (0, 'cat') in sampleOHEDictManual, \"(0, 'cat') not in sampleOHEDictManual\"\n",
    "assert (0, 'bear') in sampleOHEDictManual, \"(0, 'bear') not in sampleOHEDictManual\"\n",
    "assert (1, 'black') in sampleOHEDictManual, \"(1, 'black') not in sampleOHEDictManual\"\n",
    "assert (1, 'tabby') in sampleOHEDictManual, \"(1, 'tabby') not in sampleOHEDictManual\"\n",
    "assert (2, 'mouse') in sampleOHEDictManual, \"(2, 'mouse') not in sampleOHEDictManual\"\n",
    "assert (2, 'salmon') in sampleOHEDictManual, \"(2, 'salmon') not in sampleOHEDictManual\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1b) Vetores Esparsos **\n",
    "\n",
    "#### Pontos de dados categóricos geralmente apresentam um pequeno conjunto de OHE não-nulos relativo ao total de possíveis atributos. Tirando proveito dessa propriedade podemos representar nossos dados como vetores esparsos, economizando espaço de armazenamento e cálculos computacionais.\n",
    "\n",
    "#### No próximo exercício transforme os vetores com nome precedidos de `Dense` para vetores esparsos. Utilize a classe [SparseVector](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseVector) para representá-los e verifique que ambas as representações retornam o mesmo resultado nos cálculos dos produtos interno.\n",
    "\n",
    "#### Use `SparseVector(tamanho, *args)` para criar um novo vetor esparso onde `tamanho` é o tamanho do vetor e `args` pode ser um dicionário, uma lista de tuplas (índice, valor) ou duas arrays separadas de índices e valores ordenados por índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.mllib.linalg import SparseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.300000000000001\n",
      "7.300000000000001\n",
      "-0.5\n",
      "-0.5\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "aDense = np.array([0., 3., 0., 4.])\n",
    "aSparse = SparseVector(4, {0:0.0, 1:3.0, 2:0.0, 3:4.0})\n",
    "\n",
    "bDense = np.array([0., 0., 0., 1.])\n",
    "bSparse = SparseVector(4,[0,1,2,3],[0., 0., 0., 1.])\n",
    "\n",
    "w = np.array([0.4, 3.1, -1.4, -.5])\n",
    "print (aDense.dot(w))\n",
    "print (aSparse.dot(w))\n",
    "print (bDense.dot(w))\n",
    "print (bSparse.dot(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Sparse Vectors (1b)\n",
    "assert isinstance(aSparse, SparseVector), 'aSparse needs to be an instance of SparseVector'\n",
    "assert isinstance(bSparse, SparseVector), 'aSparse needs to be an instance of SparseVector'\n",
    "assert aDense.dot(w) == aSparse.dot(w), 'dot product of aDense and w should equal dot product of aSparse and w'\n",
    "assert bDense.dot(w) == bSparse.dot(w), 'dot product of bDense and w should equal dot product of bSparse and w'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1c) Atributos OHE como vetores esparsos **\n",
    "\n",
    "#### Agora vamos representar nossos atributos OHE como vetores esparsos. Utilizando o dicionário `sampleOHEDictManual`, crie um vetor esparso para cada amostra de nossa base de dados. Todo atributo que ocorre em uma amostra deve ter valor 1.0. Por exemplo, um vetor para um ponto com os atributos 2 e 4 devem ser `[0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder of the sample features\n",
    "# sampleOne = [(0, 'mouse'), (1, 'black')]\n",
    "# sampleTwo = [(0, 'cat'), (1, 'tabby'), (2, 'mouse')]\n",
    "# sampleThree =  [(0, 'bear'), (1, 'black'), (2, 'salmon')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "sampleOneOHEFeatManual = SparseVector(7, [(0,1.0), (1,1.0)] )\n",
    "sampleTwoOHEFeatManual = SparseVector(7, {0:1.0, 1:1.0, 2:1.0})\n",
    "sampleThreeOHEFeatManual = SparseVector(7, {0:1.0, 1:1.0, 2:1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST OHE Features as sparse vectors (1c)\n",
    "assert isinstance(sampleOneOHEFeatManual, SparseVector), 'sampleOneOHEFeatManual needs to be a SparseVector'\n",
    "assert isinstance(sampleTwoOHEFeatManual, SparseVector), 'sampleTwoOHEFeatManual needs to be a SparseVector'\n",
    "assert isinstance(sampleThreeOHEFeatManual, SparseVector), 'sampleThreeOHEFeatManual needs to be a SparseVector'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1d) Função de codificação OHE **\n",
    "\n",
    "#### Vamos criar uma função que gera um vetor esparso codificado por um dicionário de OHE. Ele deve fazer o procedimento similar ao exercício anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7,[0,1],[1.0,1.0])\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "def oneHotEncoding(rawFeats, OHEDict, numOHEFeats):\n",
    "    \"\"\"Produce a one-hot-encoding from a list of features and an OHE dictionary.\n",
    "\n",
    "    Note:\n",
    "        You should ensure that the indices used to create a SparseVector are sorted.\n",
    "\n",
    "    Args:\n",
    "        rawFeats (list of (int, str)): The features corresponding to a single observation.  Each\n",
    "            feature consists of a tuple of featureID and the feature's value. (e.g. sampleOne)\n",
    "        OHEDict (dict): A mapping of (featureID, value) to unique integer.\n",
    "        numOHEFeats (int): The total number of unique OHE features (combinations of featureID and\n",
    "            value).\n",
    "\n",
    "    Returns:\n",
    "        SparseVector: A SparseVector of length numOHEFeats with indicies equal to the unique\n",
    "            identifiers for the (featureID, value) combinations that occur in the observation and\n",
    "            with values equal to 1.0.\n",
    "    \"\"\"\n",
    "    return SparseVector(numOHEFeats, [(OHEDict[i], 1.0) for i in sorted(rawFeats, key=lambda x:x[0])])\n",
    "\n",
    "# Calculate the number of features in sampleOHEDictManual\n",
    "numSampleOHEFeats = len(sampleOHEDictManual)\n",
    "\n",
    "# Run oneHotEnoding on sampleOne\n",
    "sampleOneOHEFeat = oneHotEncoding(sampleOne, sampleOHEDictManual, numSampleOHEFeats)\n",
    "\n",
    "print (sampleOneOHEFeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Define an OHE Function (1d)\n",
    "assert sampleOneOHEFeat == sampleOneOHEFeatManual, 'sampleOneOHEFeat should equal sampleOneOHEFeatManual'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1e) Aplicar OHE em uma base de dados **\n",
    "\n",
    "#### Finalmente, use a função da parte (1d) para criar atributos OHE para todos os 3 objetos da base de dados artificial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparseVector(7, {0: 1.0, 1: 1.0}), SparseVector(7, {0: 1.0, 1: 1.0, 2: 1.0}), SparseVector(7, {0: 1.0, 1: 1.0, 2: 1.0})]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "sampleOHEData = sampleDataRDD.map(lambda x : oneHotEncoding(x,sampleOHEDictManual,numSampleOHEFeats))\n",
    "print (sampleOHEData.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Apply OHE to a dataset (1e)\n",
    "sampleOHEDataValues = sampleOHEData.collect()\n",
    "assert len(sampleOHEDataValues) == 3, 'sampleOHEData should have three elements'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 2: Construindo um dicionário OHE **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2a) Tupla RDD de `(IDatributo, categoria)` **\n",
    "\n",
    "#### Crie um RDD de pares distintos de `(IDatributo,  categoria)`. Em nossa base de dados você deve gerar `(0, 'bear')`, `(0, 'cat')`, `(0, 'mouse')`, `(1, 'black')`, `(1, 'tabby')`, `(2, 'mouse')`, `(2, 'salmon')`. Repare que `'black'` aparece duas vezes em nossa base de dados mas contribui apenas para um item do RDD: `(1, 'black')`, por outro lado `'mouse'` aparece duas vezes e contribui para dois itens: `(0, 'mouse')` and `(2, 'mouse')`.  \n",
    "\n",
    "#### Dica: use [flatMap](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.flatMap) e [distinct](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.distinct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "sampleDistinctFeats = sampleDataRDD.flatMap(lambda x: x).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Pair RDD of (featureID, category) (2a)\n",
    "assert sorted(sampleDistinctFeats.collect()) == [(0, 'bear'), (0, 'cat'), (0, 'mouse'), (1, 'black'),(1, 'tabby'), (2, 'mouse'), (2, 'salmon')], 'incorrect value for sampleDistinctFeats'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2b) Dicionário OHE de atributos únicos **\n",
    "\n",
    "#### Agora, vamos criar um RDD de tuplas para cada `(IDatributo, categoria)` em `sampleDistinctFeats`. A chave da tupla é a própria tupla original, e o valor será um inteiro variando de 0 até número de tuplas - 1. \n",
    "\n",
    "#### Em seguida, converta essa `RDD` em um dicionário, utilizando o comando `collectAsMap`. \n",
    "\n",
    "#### Use o comando  [zipWithIndex](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.zipWithIndex) seguido de [collectAsMap](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collectAsMap).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1, 'tabby'): 0, (2, 'mouse'): 1, (0, 'mouse'): 2, (2, 'salmon'): 3, (1, 'black'): 4, (0, 'bear'): 5, (0, 'cat'): 6}\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "sampleOHEDict = sampleDistinctFeats.zipWithIndex().collectAsMap()\n",
    "print (sampleOHEDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST OHE Dictionary from distinct features (2b)\n",
    "assert sorted(sampleOHEDict.keys()) == [(0, 'bear'), (0, 'cat'), (0, 'mouse'), (1, 'black'),(1, 'tabby'), (2, 'mouse'), (2, 'salmon')], 'sampleOHEDict has unexpected keys'\n",
    "assert sorted(sampleOHEDict.values()) == list(range(7)), 'sampleOHEDict has unexpected values'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2c) Criação automática do dicionário OHE **\n",
    "\n",
    "#### Agora use os códigos dos exercícios anteriores para criar uma função que retorna um dicionário OHE a partir dos atributos categóricos de uma base de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1, 'tabby'): 0, (2, 'mouse'): 1, (0, 'mouse'): 2, (2, 'salmon'): 3, (1, 'black'): 4, (0, 'bear'): 5, (0, 'cat'): 6}\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "def createOneHotDict(inputData):\n",
    "    \"\"\"Creates a one-hot-encoder dictionary based on the input data.\n",
    "\n",
    "    Args:\n",
    "        inputData (RDD of lists of (int, str)): An RDD of observations where each observation is\n",
    "            made up of a list of (featureID, value) tuples.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are (featureID, value) tuples and map to values that are\n",
    "            unique integers.\n",
    "    \"\"\"\n",
    "    return inputData.flatMap(lambda x: x).distinct().zipWithIndex().collectAsMap()\n",
    "\n",
    "sampleOHEDictAuto = createOneHotDict(sampleDataRDD)\n",
    "print (sampleOHEDictAuto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Automated creation of an OHE dictionary (2c)\n",
    "assert sorted(sampleOHEDictAuto.keys()) == [(0, 'bear'), (0, 'cat'), (0, 'mouse'), (1, 'black'), (1, 'tabby'), (2, 'mouse'), (2, 'salmon')], 'sampleOHEDictAuto has unexpected keys'\n",
    "assert sorted(sampleOHEDictAuto.values()) == list(range(7)), 'sampleOHEDictAuto has unexpected values'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 3: Parse CTR data and generate OHE features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Antes de começar essa parte, vamos carregar a base de dados e verificar o formato dela.\n",
    "\n",
    "#### Repare que o primeiro campo é o rótulo de cada objeto, sendo 0 se o usuário não clicou no banner e 1 caso tenha clicado. O restante dos atributos ou são numéricos ou são strings representando categorias anônimas. Vamos tratar todos os atributos como categóricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0,1,1,5,0,1382,4,15,2,181,1,2,,2,68fd1e64,80e26c9b,fb936136,7b4723c4,25c83c98,7e0ccccf,de7995b8,1f89b562,a73ee510,a8cd5504,b2cb9c98,37c9c164,2824a5f6,1adce6ef,8ba8b39a,891b62e7,e5ba7672,f54016b9,21ddcdc9,b1252a9d,07b5194c,,3a171ecb,c5c50484,e8b83407,9727dd16']\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "fileName = os.path.join('Data', 'dac_sample.txt')\n",
    "\n",
    "if os.path.isfile(fileName):\n",
    "    rawData = (sc\n",
    "               .textFile(fileName, 2)\n",
    "               .map(lambda x: x.replace('\\t', ',')))  # work with either ',' or '\\t' separated data\n",
    "    print(rawData.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3a) Carregando e dividindo os dados **\n",
    "\n",
    "#### Da mesma forma que no notebook anterior, vamos dividir os dados entre treinamento, validação e teste. Use o método [randomSplit](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.randomSplit) com os pesos (weights) e semente aleatória (seed) especificados para criar os conjuntos, então faça o [cache](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.cache) de cada RDD, pois utilizaremos cada uma delas com frequência durante esse exercício."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80053 9941 10006 100000\n",
      "['0,1,1,5,0,1382,4,15,2,181,1,2,,2,68fd1e64,80e26c9b,fb936136,7b4723c4,25c83c98,7e0ccccf,de7995b8,1f89b562,a73ee510,a8cd5504,b2cb9c98,37c9c164,2824a5f6,1adce6ef,8ba8b39a,891b62e7,e5ba7672,f54016b9,21ddcdc9,b1252a9d,07b5194c,,3a171ecb,c5c50484,e8b83407,9727dd16']\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "# Use randomSplit with weights and seed\n",
    "rawTrainData, rawValidationData, rawTestData = rawData.randomSplit(weights, seed)\n",
    "# Cache the data\n",
    "rawTrainData.cache().randomSplit(weights)\n",
    "rawValidationData.cache().randomSplit(weights, seed)\n",
    "rawTestData.cache()\n",
    "\n",
    "nTrain = rawTrainData.count()\n",
    "nVal = rawValidationData.count()\n",
    "nTest = rawTestData.count()\n",
    "print (nTrain, nVal, nTest, nTrain + nVal + nTest)\n",
    "print (rawData.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Loading and splitting the data (3a)\n",
    "assert all([rawTrainData.is_cached, rawValidationData.is_cached, rawTestData.is_cached]), 'you must cache the split data'\n",
    "assert nTrain == 80053, 'incorrect value for nTrain'\n",
    "assert nVal == 9941, 'incorrect value for nVal'\n",
    "assert nTest == 10006, 'incorrect value for nTest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (3b) Extração de atributos **\n",
    "\n",
    "#### Como próximo passo, crie uma função para ser aplicada em cada objeto do RDD para gerar uma RDD de tuplas (IDatributo, categoria). Ignore o primeiro campo, que é o rótulo e gere uma lista de tuplas para os atributos seguintes. Utilize o comando [enumerate](https://docs.python.org/2/library/functions.html#enumerate) para criar essas tuplas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "864\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "def parsePoint(point):\n",
    "    \"\"\"Converts a comma separated string into a list of (featureID, value) tuples.\n",
    "\n",
    "    Note:\n",
    "        featureIDs should start at 0 and increase to the number of features - 1.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (featureID, value) tuples.\n",
    "    \"\"\"\n",
    "    return list(enumerate(point.split(',')[1:]))\n",
    "\n",
    "parsedTrainFeat = rawTrainData.map(parsePoint)\n",
    "\n",
    "numCategories = (parsedTrainFeat\n",
    "                 .flatMap(lambda x: x)\n",
    "                 .distinct()\n",
    "                 .map(lambda x: (x[0], 1))\n",
    "                 .reduceByKey(lambda x, y: x + y)\n",
    "                 .sortByKey()\n",
    "                 .collect()\n",
    "                )\n",
    "\n",
    "print (numCategories[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Extract features (3b)\n",
    "assert numCategories[2][1] == 864, 'incorrect implementation of parsePoint'\n",
    "assert numCategories[32][1] == 4, 'incorrect implementation of parsePoint'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3c) Crie o dicionário de OHE dessa base de dados **\n",
    "\n",
    "#### Note que a função parsePoint retorna um objeto em forma de lista `(IDatributo, categoria)`, que é o mesmo formato utilizado pela função `createOneHotDict`. Utilize o RDD `parsedTrainFeat` para criar um dicionário OHE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234358\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "ctrOHEDict = createOneHotDict(parsedTrainFeat)\n",
    "numCtrOHEFeats = len(ctrOHEDict.keys())\n",
    "print (numCtrOHEFeats)\n",
    "print (ctrOHEDict[(0, '')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Create an OHE dictionary from the dataset (3c)\n",
    "assert numCtrOHEFeats == 234358, 'incorrect number of features in ctrOHEDict'\n",
    "assert (0, '') in ctrOHEDict, 'incorrect features in ctrOHEDict'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (3d) Aplicando OHE à base de dados **\n",
    "\n",
    "#### Agora vamos usar o dicionário OHE para criar um RDD de objetos [LabeledPoint](http://spark.apache.org/docs/1.3.1/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint) usando atributos OHE. Complete a função `parseOHEPoint`. Dica: essa função é uma extensão da função `parsePoint` criada anteriormente e que usa a função `oneHotEncoding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, (234358,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,117215,117216,117217,117218,117219,117220,117221,117222,117223,117224,117225,117226,117227,117228,117229,117230,117231,117232,117233,117234,117235],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "def parseOHEPoint(point, OHEDict, numOHEFeats):\n",
    "    \"\"\"Obtain the label and feature vector for this raw observation.\n",
    "\n",
    "    Note:\n",
    "        You must use the function `oneHotEncoding` in this implementation or later portions\n",
    "        of this lab may not function as expected.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "        OHEDict (dict of (int, str) to int): Mapping of (featureID, value) to unique integer.\n",
    "        numOHEFeats (int): The number of unique features in the training dataset.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: Contains the label for the observation and the one-hot-encoding of the\n",
    "            raw features based on the provided OHE dictionary.\n",
    "    \"\"\"\n",
    "    return LabeledPoint(point.split(',')[0], oneHotEncoding(parsePoint(point), OHEDict, numOHEFeats))\n",
    "    \n",
    "OHETrainData = rawTrainData.map(lambda point: parseOHEPoint(point, ctrOHEDict, numCtrOHEFeats))\n",
    "OHETrainData.cache()\n",
    "print (OHETrainData.take(1))\n",
    "\n",
    "# Check that oneHotEncoding function was used in parseOHEPoint\n",
    "backupOneHot = oneHotEncoding\n",
    "oneHotEncoding = None\n",
    "withOneHot = False\n",
    "try: parseOHEPoint(rawTrainData.take(1)[0], ctrOHEDict, numCtrOHEFeats)\n",
    "except TypeError: withOneHot = True\n",
    "oneHotEncoding = backupOneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Apply OHE to the dataset (3d)\n",
    "numNZ = sum(parsedTrainFeat.map(lambda x: len(x)).take(5))\n",
    "numNZAlt = sum(OHETrainData.map(lambda lp: len(lp.features.indices)).take(5))\n",
    "assert numNZ == numNZAlt, 'incorrect implementation of parseOHEPoint'\n",
    "assert withOneHot, 'oneHotEncoding not present in parseOHEPoint'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visualização 1: Frequência dos Atributos **\n",
    "\n",
    "#### Vamos agora visualizar o número de vezes que cada um dos 233.286 atributos OHE aparecem na base de treino.  Para isso primeiro contabilizamos quantas vezes cada atributo aparece na base, então alocamos cada atributo em um balde de histograma. Os baldes tem tamanhos de potência de 2, então o primeiro balde conta os atributos que aparecem exatamente uma vez ( $ \\scriptsize 2^0 $ ), o segundo atributos que aparecem duas vezes ( $ \\scriptsize 2^1 $ ), o terceiro os atributos que aparecem de 3 a 4 vezes ( $ \\scriptsize 2^2 $ ), o quinto balde é para atributos que ocorrem de cinco a oito vezes ( $ \\scriptsize 2^3 $ ) e assim por diante. O gráfico de dispersão abaixo mostra o logarítmo do tamanho dos baldes versus o logarítmo da frequência de atributos que caíram nesse balde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, 11480), (2, 23578), (16, 7750), (128, 1504), (64, 2663), (256, 733), (32, 4792), (512, 416), (1024, 251), (4, 16870), (1, 164030)]\n"
     ]
    }
   ],
   "source": [
    "def bucketFeatByCount(featCount):\n",
    "    \"\"\"Bucket the counts by powers of two.\"\"\"\n",
    "    for i in range(11):\n",
    "        size = 2 ** i\n",
    "        if featCount <= size:\n",
    "            return size\n",
    "    return -1\n",
    "\n",
    "featCounts = (OHETrainData\n",
    "              .flatMap(lambda lp: lp.features.indices)\n",
    "              .map(lambda x: (x, 1))\n",
    "              .reduceByKey(lambda x, y: x + y))\n",
    "featCountsBuckets = (featCounts\n",
    "                     .map(lambda x: (bucketFeatByCount(x[1]), 1))\n",
    "                     .filter(lambda kv: kv[0] != -1)\n",
    "                     .reduceByKey(lambda x, y: x + y)\n",
    "                     .collect())\n",
    "print (featCountsBuckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7964adb668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "x, y = zip(*featCountsBuckets)\n",
    "x, y = np.log(x), np.log(y)\n",
    "\n",
    "def preparePlot(xticks, yticks, figsize=(10.5, 6), hideLabels=False, gridColor='#999999',\n",
    "                gridWidth=1.0):\n",
    "    \"\"\"Template for generating the plot layout.\"\"\"\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=figsize, facecolor='white', edgecolor='white')\n",
    "    ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n",
    "    for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n",
    "        axis.set_ticks_position('none')\n",
    "        axis.set_ticks(ticks)\n",
    "        axis.label.set_color('#999999')\n",
    "        if hideLabels: axis.set_ticklabels([])\n",
    "    plt.grid(color=gridColor, linewidth=gridWidth, linestyle='-')\n",
    "    map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n",
    "    return fig, ax\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = preparePlot(np.arange(0, 10, 1), np.arange(4, 14, 2))\n",
    "ax.set_xlabel(r'$\\log_e(bucketSize)$'), ax.set_ylabel(r'$\\log_e(countInBucket)$')\n",
    "plt.scatter(x, y, s=14**2, c='#d6ebf2', edgecolors='#8cbfd0', alpha=0.75)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3e) Atributos não observados **\n",
    "\n",
    "#### Naturalmente precisaremos aplicar esse mesmo procedimento para as outras bases (validação e teste), porém nessas bases podem existir atributos não observados na base de treino.\n",
    "\n",
    "#### Precisamos adaptar a função `oneHotEncoding` para ignorar os atributos que não existem no dicionário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, (234358,[2,8,15,21,45,49,50,52,61,96,135,160,164,671,1355,5140,5141,5142,117223,117226,117238,117242,117258,117261,117262,117267,117269,117284,117287,118378,119236,120375,122236,122237,122238,122239,122240,122241,170030],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "def oneHotEncoding(rawFeats, OHEDict, numOHEFeats):\n",
    "    \"\"\"Produce a one-hot-encoding from a list of features and an OHE dictionary.\n",
    "\n",
    "    Note:\n",
    "        If a (featureID, value) tuple doesn't have a corresponding key in OHEDict it should be\n",
    "        ignored.\n",
    "\n",
    "    Args:\n",
    "        rawFeats (list of (int, str)): The features corresponding to a single observation.  Each\n",
    "            feature consists of a tuple of featureID and the feature's value. (e.g. sampleOne)\n",
    "        OHEDict (dict): A mapping of (featureID, value) to unique integer.\n",
    "        numOHEFeats (int): The total number of unique OHE features (combinations of featureID and\n",
    "            value).\n",
    "\n",
    "    Returns:\n",
    "        SparseVector: A SparseVector of length numOHEFeats with indicies equal to the unique\n",
    "            identifiers for the (featureID, value) combinations that occur in the observation and\n",
    "            with values equal to 1.0.\n",
    "    \"\"\"\n",
    "    vector = []\n",
    "    for i in sorted(rawFeats, key=lambda x:x[0]):\n",
    "        if i in OHEDict: \n",
    "            vector.append( (OHEDict[i], 1.0) )\n",
    "    return SparseVector(numOHEFeats, vector)\n",
    "\n",
    "OHEValidationData = rawValidationData.map(lambda point: parseOHEPoint(point, ctrOHEDict, numCtrOHEFeats))\n",
    "OHEValidationData.cache()\n",
    "print (OHEValidationData.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Handling unseen features (3e)\n",
    "numNZVal = (OHEValidationData\n",
    "            .map(lambda lp: len(lp.features.indices))\n",
    "            .sum())\n",
    "assert numNZVal == 367585, 'incorrect number of features'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 4: Predição do CTR e avaliação da perda-log (logloss) **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4a) Regressão Logística **\n",
    "\n",
    "#### Um classificador que podemos utilizar nessa base de dados é a regressão logística, que nos dá a probabilidade de um evento de clique em banner ocorrer. Vamos utilizar a função  [LogisticRegressionWithSGD](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.classification.LogisticRegressionWithSGD) para treinar um modelo usando  `OHETrainData` com a configuração de parâmetros dada.  `LogisticRegressionWithSGD` retorna um [LogisticRegressionModel](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LogisticRegressionModel).  \n",
    "\n",
    "#### Em seguida, imprima  `LogisticRegressionModel.weights` e `LogisticRegressionModel.intercept` para verificar o modelo gerado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "\n",
    "# fixed hyperparameters\n",
    "numIters = 50\n",
    "stepSize = 10.\n",
    "regParam = 1e-6\n",
    "regType = 'l2'\n",
    "includeIntercept = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.4182118127233312, -0.3909505990206721, -0.34429134263547495, -0.33736877269560583, -0.3318652383680194] 0.5561590969787334\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "model0 = LogisticRegressionWithSGD.train(data=OHEValidationData,\n",
    "                                         iterations=numIters,\n",
    "                                         step=stepSize,\n",
    "                                         regParam=regParam,\n",
    "                                         regType=regType,\n",
    "                                         intercept=includeIntercept)\n",
    "sortedWeights = sorted(model0.weights)\n",
    "print (sortedWeights[:5], model0.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "incorrect value for model0.intercept",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-432c632d6dd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TEST Logistic regression (4a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.5616041364601837\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'incorrect value for model0.intercept'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msortedWeights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.46297159426279577\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.39040230182817892\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.3871281985827924\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.35815003494268316\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.34963241495474701\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'incorrect value for model0.weights'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: incorrect value for model0.intercept"
     ]
    }
   ],
   "source": [
    "# TEST Logistic regression (4a)\n",
    "assert np.allclose(model0.intercept,  0.5616041364601837), 'incorrect value for model0.intercept'\n",
    "assert np.allclose(sortedWeights[0:5], [-0.46297159426279577, -0.39040230182817892, -0.3871281985827924, -0.35815003494268316, -0.34963241495474701]), 'incorrect value for model0.weights'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4b) Log loss **\n",
    "\n",
    "#### Uma forma de avaliar um classificador binário é através do log-loss, definido como: $$  \\begin{align} \\scriptsize \\ell_{log}(p, y) = \\begin{cases} -\\log (p) & \\text{if } y = 1 \\\\\\ -\\log(1-p) & \\text{if } y = 0 \\end{cases} \\end{align} $$ onde $ \\scriptsize p$ é uma probabilidade entre 0 e 1 e  $ \\scriptsize y$ é o rótulo binário (0 ou 1). Log loss é um critério de avaliação muito utilizado quando deseja-se predizer eventos raros. Escreva uma função para calcular o log-loss, e avalie algumas entradas de amostra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599453\n",
      "0.6931471805599453\n",
      "0.01005033585350145\n",
      "4.605170185988091\n",
      "4.605170185988091\n",
      "0.01005033585350145\n",
      "25.328436022934504\n",
      "1.000000082745371e-11\n",
      "25.328435940194137\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "import numpy as np\n",
    "\n",
    "def computeLogLoss(p, y):\n",
    "    \"\"\"Calculates the value of log loss for a given probabilty and label.\n",
    "\n",
    "    Note:\n",
    "        log(0) is undefined, so when p is 0 we need to add a small value (epsilon) to it\n",
    "        and when p is 1 we need to subtract a small value (epsilon = 1e-11) from it.\n",
    "\n",
    "    Args:\n",
    "        p (float): A probabilty between 0 and 1.\n",
    "        y (int): A label.  Takes on the values 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        float: The log loss value.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-11\n",
    "    if p == 0:\n",
    "        p = epsilon\n",
    "    elif p == 1:\n",
    "        p -= epsilon\n",
    "    if y == 1:\n",
    "        return (-1)*np.log(p)\n",
    "    else: # y == 0\n",
    "        return (-1)*np.log(1-p)\n",
    "\n",
    "print (computeLogLoss(.5, 1))\n",
    "print( computeLogLoss(.5, 0))\n",
    "print (computeLogLoss(.99, 1))\n",
    "print (computeLogLoss(.99, 0))\n",
    "print (computeLogLoss(.01, 1))\n",
    "print (computeLogLoss(.01, 0))\n",
    "print (computeLogLoss(0, 1))\n",
    "print (computeLogLoss(1, 1))\n",
    "print (computeLogLoss(1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Log loss (4b)\n",
    "assert np.allclose([computeLogLoss(.5, 1), computeLogLoss(.01, 0), computeLogLoss(.01, 1)], [0.69314718056, 0.0100503358535, 4.60517018599]), 'computeLogLoss is not correct'\n",
    "assert np.allclose([computeLogLoss(0, 1), computeLogLoss(1, 1), computeLogLoss(1, 0)], [25.3284360229, 1.00000008275e-11, 25.3284360229]), 'computeLogLoss needs to bound p away from 0 and 1 by epsilon'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4c)  Baseline log loss **\n",
    "\n",
    "#### Agora, vamos utilizar a função da Parte (4b) para calcular um baseline da métrica de log-loss na nossa base de treino. Uma forma de calcular um baseline é predizer sempre a média dos rótulos observados. Primeiro calcule a média dos rótulos da base e, em seguida, calcule o log-loss médio para a base de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 155.0 failed 1 times, most recent failure: Lost task 0.0 in stage 155.0 (TID 331, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 362, in func\n    return f(iterator)\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1066, in <lambda>\n    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/statcounter.py\", line 42, in __init__\n    for v in values:\n  File \"<ipython-input-58-d9cec4bad3cd>\", line 7, in <lambda>\nTypeError: computeLogLoss() got an unexpected keyword argument 'x'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor58.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 362, in func\n    return f(iterator)\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1066, in <lambda>\n    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/statcounter.py\", line 42, in __init__\n    for v in values:\n  File \"<ipython-input-58-d9cec4bad3cd>\", line 7, in <lambda>\nTypeError: computeLogLoss() got an unexpected keyword argument 'x'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-d9cec4bad3cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print (classOneFracTrain)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlogLossTrBase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOHETrainData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mlp\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mcomputeLogLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#<COMPLETAR>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Baseline Train Logloss = {0:.3f}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogLossTrBase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;36m2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m         \"\"\"\n\u001b[0;32m-> 1203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mstats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1064\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mleft_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmergeStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStatCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    848\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \"\"\"\n\u001b[1;32m    823\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 155.0 failed 1 times, most recent failure: Lost task 0.0 in stage 155.0 (TID 331, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 362, in func\n    return f(iterator)\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1066, in <lambda>\n    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/statcounter.py\", line 42, in __init__\n    for v in values:\n  File \"<ipython-input-58-d9cec4bad3cd>\", line 7, in <lambda>\nTypeError: computeLogLoss() got an unexpected keyword argument 'x'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor58.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 362, in func\n    return f(iterator)\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1066, in <lambda>\n    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)\n  File \"/home/pedro/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/statcounter.py\", line 42, in __init__\n    for v in values:\n  File \"<ipython-input-58-d9cec4bad3cd>\", line 7, in <lambda>\nTypeError: computeLogLoss() got an unexpected keyword argument 'x'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "# Note that our dataset has a very high click-through rate by design\n",
    "# In practice click-through rate can be one to two orders of magnitude lower\n",
    "classOneFracTrain = OHETrainData.mean()#<COMPLETAR>\n",
    "print (classOneFracTrain)\n",
    "\n",
    "logLossTrBase = OHETrainData.map(lambda lp : computeLogLoss(x=lp.features, y=lp.label)).mean()#<COMPLETAR>\n",
    "print ('Baseline Train Logloss = {0:.3f}\\n'.format(logLossTrBase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST Baseline log loss (4c)\n",
    "assert np.allclose(classOneFracTrain, 0.2271245299988764), 'incorrect value for classOneFracTrain'\n",
    "assert np.allclose(logLossTrBase, 0.535778466496), 'incorrect value for logLossTrBase'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4d) Probabilidade da Predição **\n",
    "\n",
    "#### O modelo gerado na Parte (4a) possui um método chamado `predict`, porém esse método retorna apenas 0's e 1's. Para calcular a probabilidade de um evento, vamos criar uma função `getP` que recebe como parâmetro o ponto x, o conjunto de pesos `w` e o `intercept`.\n",
    "\n",
    "#### Calcule o modelo de regressão linear nesse ponto x e aplique a  [função sigmoidal](http://en.wikipedia.org/wiki/Sigmoid_function) $ \\scriptsize \\sigma(t) = (1+ e^{-t})^{-1} $ para retornar a probabilidade da predição do objeto x.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "from math import exp #  exp(-t) = e^-t\n",
    "\n",
    "def getP(x, w, intercept):\n",
    "    \"\"\"Calculate the probability for an observation given a set of weights and intercept.\n",
    "\n",
    "    Note:\n",
    "        We'll bound our raw prediction between 20 and -20 for numerical purposes.\n",
    "\n",
    "    Args:\n",
    "        x (SparseVector): A vector with values of 1.0 for features that exist in this\n",
    "            observation and 0.0 otherwise.\n",
    "        w (DenseVector): A vector of weights (betas) for the model.\n",
    "        intercept (float): The model's intercept.\n",
    "\n",
    "    Returns:\n",
    "        float: A probability between 0 and 1.\n",
    "    \"\"\"\n",
    "    # calculate rawPrediction = w.x + intercept\n",
    "    rawPrediction = w.dot(x) + intercept #<COMPLETAR>\n",
    "\n",
    "    # Bound the raw prediction value\n",
    "    rawPrediction = min(rawPrediction, 20)\n",
    "    rawPrediction = max(rawPrediction, -20)\n",
    "    \n",
    "    # calculate (1+e^-rawPrediction)^-1\n",
    "    return 1+exp((-1)*rawPrediction)**(-1)#<COMPLETAR>\n",
    "\n",
    "trainingPredictions = OHETrainData.map(lambda lp:  )#<COMPLETAR>\n",
    "\n",
    "print (trainingPredictions.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST Predicted probability (4d)\n",
    "assert np.allclose(trainingPredictions.sum(), 18198.8525175), 'incorrect value for trainingPredictions'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4e) Avalie o modelo **\n",
    "\n",
    "#### Finalmente, crie uma função `evaluateResults` que calcula o log-loss médio do modelo em uma base de dados. Em seguida, execute essa função na nossa base de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "def evaluateResults(model, data):\n",
    "    \"\"\"Calculates the log loss for the data given the model.\n",
    "\n",
    "    Args:\n",
    "        model (LogisticRegressionModel): A trained logistic regression model.\n",
    "        data (RDD of LabeledPoint): Labels and features for each observation.\n",
    "\n",
    "    Returns:\n",
    "        float: Log loss for the data.\n",
    "    \"\"\"\n",
    "    return (data\n",
    "            .<COMPLETAR>\n",
    "            .<COMPLETAR>\n",
    "            .<COMPLETAR>\n",
    "            )\n",
    "\n",
    "logLossTrLR0 = evaluateResults(model0, OHETrainData)\n",
    "print ('OHE Features Train Logloss:\\n\\tBaseline = {0:.3f}\\n\\tLogReg = {1:.3f}'.format(logLossTrBase, logLossTrLR0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST Evaluate the model (4e)\n",
    "assert np.allclose(logLossTrLR0, 0.45704573867), 'incorrect value for logLossTrLR0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4f) log-loss da validação **\n",
    "\n",
    "#### Agora aplique o modelo na nossa base de validação e calcule o log-loss médio, compare com o nosso baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "logLossValBase = OHEValidationData.<COMPLETAR>\n",
    "\n",
    "logLossValLR0 = evaluateResults(model0, OHEValidationData)\n",
    "print ('OHE Features Validation Logloss:\\n\\tBaseline = {0:.3f}\\n\\tLogReg = {1:.3f}'.format(logLossValBase, logLossValLR0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST Validation log loss (4f)\n",
    "assert np.allclose(logLossValBase, 0.526558409461), 'incorrect value for logLossValBase'\n",
    "assert np.allclose(logLossValLR0, 0.458434994198), 'incorrect value for logLossValLR0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visualização 2: Curva ROC  **\n",
    "\n",
    "#### A curva ROC nos mostra o custo-benefício entre a taxa de falso positivo e a taxa de verdadeiro positivo, conforme diminuimos o limiar de predição. Um modelo aleatório é representado por uma linha pontilhada. Idealmente nosso modelo deve formar uma curva acima dessa linha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelsAndScores = OHEValidationData.map(lambda lp:\n",
    "                                            (lp.label, getP(lp.features, model0.weights, model0.intercept)))\n",
    "labelsAndWeights = labelsAndScores.collect()\n",
    "labelsAndWeights.sort(key=lambda kv: kv[1], reverse=True)\n",
    "labelsByWeight = np.array([k for (k, v) in labelsAndWeights])\n",
    "\n",
    "length = labelsByWeight.size\n",
    "truePositives = labelsByWeight.cumsum()\n",
    "numPositive = truePositives[-1]\n",
    "falsePositives = np.arange(1.0, length + 1, 1.) - truePositives\n",
    "\n",
    "truePositiveRate = truePositives / numPositive\n",
    "falsePositiveRate = falsePositives / (length - numPositive)\n",
    "\n",
    "# Generate layout and plot data\n",
    "fig, ax = preparePlot(np.arange(0., 1.1, 0.1), np.arange(0., 1.1, 0.1))\n",
    "ax.set_xlim(-.05, 1.05), ax.set_ylim(-.05, 1.05)\n",
    "ax.set_ylabel('True Positive Rate (Sensitivity)')\n",
    "ax.set_xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.plot(falsePositiveRate, truePositiveRate, color='#8cbfd0', linestyle='-', linewidth=3.)\n",
    "plt.plot((0., 1.), (0., 1.), linestyle='--', color='#d6ebf2', linewidth=2.)  # Baseline model\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
